# Implementation Plan

## ðŸ“Š Progress Summary
**Completed:** 6/11 tasks (55%)  
**Status:** Real Data Shapley API integration complete - ready for NEAR rewards  
**Latest Update:** September 28, 2025 - PR #76 under review

### âœ… Major Accomplishments
- **Complete ML Pipeline**: fNIRS processing, glucose prediction, real Data Shapley
- **API Integration**: Real Shapley calculations in production endpoint
- **Two-Session Simulation**: User data tested against both research sessions
- **Production Ready**: ~3 second API response time with comprehensive testing

### ðŸŽ¯ Next Priority: NEAR Token Integration
Ready to implement Dream Dew token rewards based on Shapley contribution scores.

- [x] 1. Set up Python ML pipeline infrastructure
  - Create ml_pipeline.py module in agent_logic directory with FastAPI endpoints
  - Set up proper Python environment with ML dependencies (numpy, pandas, scikit-learn)
  - Create data models for fNIRS processing requests and responses
  - _Requirements: 2.1, 2.2, 2.5_

- [ ] 2. Create fNIRS data processing pipeline from scratch
  - Implement fNIRS data preprocessing functions (CSV parsing, validation, noise filtering)
  - Create dHbO/dHbR conversion algorithms using extinction coefficients
  - Implement epoch-based feature extraction (mean, std, skew, kurtosis, max_minus_min)
  - Add data quality assessment metrics (signal-to-noise ratio, completeness checks)
  - _Requirements: 2.1, 2.2, 2.3_

- [ ] 3. Implement glucose prediction models for scoring
  - Create basic regression models (RandomForest, Ridge) for glucose prediction from fNIRS features
  - Implement model training pipeline with synthetic or sample data
  - Add prediction confidence scoring based on model ensemble agreement
  - Create model persistence (save/load) functionality
  - _Requirements: 2.3_

- [ ] 4. Enhance Shapley scoring algorithm with real ML integration
  - Expand existing shapley_scorer.py to use actual fNIRS processing results instead of mocks
  - Integrate data quality metrics from the ML pipeline (signal-to-noise, epoch count, feature completeness)
  - Use real glucose prediction accuracy as part of the contribution scoring
  - Implement the deterministic scoring rules from the Genkit prompt but with actual ML metrics
  - Add Shapley value calculation based on marginal contribution to model performance
  - Write unit tests to ensure scoring consistency and determinism
  - _Requirements: 3.1, 3.2, 3.3_

- [x] 5. Fix Python API endpoint for data contribution scoring
  - âœ… Integrate real Shapley scorer into /ml/api/score-contribution endpoint
  - âœ… Replace heuristic scoring with actual Data Shapley calculations
  - âœ… Implement two-session simulation (Session 1 vs Session 2)
  - âœ… Add within-session coalition sampling for user data contribution
  - âœ… Proper error handling with fallback to heuristic scoring
  - âœ… Comprehensive API testing with real fNIRS data samples
  - _Requirements: 2.4, 5.1, 5.2_
  - _Completed in: PR #76, feat/integrate-shapley-api branch_

- [ ] 6. Replace Genkit AI integration in frontend
  - Update handleDataContribution function in src/app/page.tsx to call Python API
  - Remove dependency on score-data-contribution-flow.ts Genkit flow
  - Update error handling to work with new API response format
  - Modify progress tracking to reflect Python processing stages
  - _Requirements: 7.1, 7.2, 7.3, 7.4_

- [ ] 7. Add missing approve_bonus function to staking contract
  - Implement approve_bonus function in contracts/staking_contract/src/lib.rs
  - Add agent authorization and bonus approval logic
  - Update contract to handle agent-initiated bonus approvals
  - Test the integration with the existing /api/verify-rest endpoint
  - _Requirements: 6.1, 6.2_

- [ ] 8. Implement NEAR token payment system for Swarm storage
  - Extend staking contract with storage payment functions (initiate_storage_payment, confirm_storage_payment)
  - Create bridge service module for NEAR to Gnosis chain token transfers
  - Add token balance validation and payment flow in frontend
  - Implement cross-chain transaction monitoring and confirmation
  - _Requirements: 6.1, 6.2, 6.3, 6.4_

- [ ] 9. Complete Swarm storage integration with postage stamp purchasing
  - Implement postage stamp purchase functionality using bridged tokens
  - Add client-side data encryption before Swarm upload
  - Complete SwarmStorage component backend integration for file uploads
  - Create Swarm hash storage and retrieval system
  - Connect SwarmStorage component to actual backend endpoints
  - _Requirements: 4.1, 4.2, 4.3, 4.5_

- [ ] 10. Add comprehensive logging and monitoring system
  - Implement detailed logging for all ML processing steps
  - Add performance metrics tracking (processing time, memory usage)
  - Create error logging with stack traces and context information
  - Add API response time and success rate monitoring
  - _Requirements: 5.1, 5.2, 5.3, 5.4_

- [ ] 11. Create end-to-end integration tests
  - Write tests for complete data flow from upload to Swarm storage
  - Test error handling scenarios (invalid data, payment failures, storage errors)
  - Create performance tests for ML pipeline under load
  - Add security tests for input validation and data encryption
  - _Requirements: 1.1, 1.2, 1.3, 1.4, 2.4, 4.6_